---
title: "How to Read Election Polls: Margin of Error, Sample Size, and Statistical Confidence"
excerpt: "A comprehensive guide to understanding polling methodology, interpreting results, and recognizing quality surveys in Nepal's electoral context."
author: "Methodology Team"
date: "2026-02-11"
category: "Methodology"
tags: ["polling", "methodology", "statistics", "data-literacy"]
featuredImage: "/images/election-polls.svg"
featured: true
---

# How to Read Election Polls: Margin of Error, Sample Size, and Statistical Confidence

Election polls flood media coverage during campaign season, but few voters understand what the numbers actually mean. A poll showing "NC: 32%, UML: 28%" seems straightforward—until you see the fine print: "±3.5%, 95% confidence."

What does that mean? And why should you care?

This guide explains polling methodology, helping you distinguish reliable surveys from statistical noise.

## The Basics: What Polls Measure

Election polls attempt to estimate voting intentions for an entire population (Nepal's 17.9 million eligible voters) by surveying a small sample (typically 1,000-3,000 respondents).

**Key insight**: Polls don't predict the future—they measure current sentiment with quantifiable uncertainty.

### The Three Critical Numbers

Every reputable poll reports three numbers:

1. **Point estimate** (e.g., "NC: 32%")
2. **Margin of error** (e.g., "±3.5%")
3. **Confidence level** (e.g., "95%")

Together, these mean: "We are 95% confident that NC's true support lies between 28.5% and 35.5%."

## Sample Size: Why 1,000 Is Usually Enough

**Common question**: "How can 1,000 people represent 18 million voters?"

**Answer**: Mathematics.

The margin of error depends primarily on sample size, not population size. The formula:

```
Margin of Error = 1.96 × √(p(1-p)/n)
```

Where:
- `p` = proportion (e.g., 0.32 for 32%)
- `n` = sample size
- `1.96` = z-score for 95% confidence

**Sample size vs. margin of error:**

| Sample Size | Margin of Error (95% CI) |
|-------------|--------------------------|
| 400         | ±4.9%                    |
| 600         | ±4.0%                    |
| 1,000       | ±3.1%                    |
| 1,500       | ±2.5%                    |
| 2,500       | ±2.0%                    |
| 4,000       | ±1.5%                    |

**Notice**: Doubling sample size from 1,000 to 2,000 only reduces error from 3.1% to 2.2%. Quadrupling to 4,000 gets you to 1.5%. Diminishing returns make samples beyond 2,000-3,000 rarely cost-effective.

### Why This Works

Statistical sampling relies on the **Law of Large Numbers**: random samples converge toward the true population value as sample size increases.

**Critical assumption**: The sample must be **random and representative**. This is where most polls fail.

## Margin of Error: Interpreting the Range

The margin of error creates a **confidence interval** around each estimate.

**Example poll result:**
- NC: 32% ± 3.5%
- UML: 28% ± 3.5%
- Maoist: 15% ± 3.5%
- RSP: 12% ± 3.5%

**Confidence intervals:**
- NC: 28.5% to 35.5%
- UML: 24.5% to 31.5%
- Maoist: 11.5% to 18.5%
- RSP: 8.5% to 15.5%

**Key insight**: The ranges overlap! NC and UML could be tied (both at ~30%), or NC could be ahead by 11 points (35.5% vs 24.5%).

**Statistical tie**: When confidence intervals overlap significantly, the race is too close to call.

### What "95% Confidence" Means

"95% confidence level" means: If we repeated this survey 100 times with different random samples, 95 of those surveys would produce results within the margin of error of the true value.

**It does NOT mean**: "There's a 95% chance NC is between 28.5% and 35.5%."

The true value is fixed (we just don't know it). The confidence interval is our uncertainty.

**5% of the time**, random chance alone will produce results outside the margin of error. This is why "outlier polls" happen.

## Sampling Methodology: The Make-or-Break Factor

Sample size determines precision, but **sampling methodology** determines accuracy.

### Random Digit Dialing (RDD)

**Method**: Computer generates random phone numbers, calls respondents.

**Pros**: True randomization, wide geographic coverage

**Cons**:
- Excludes people without phones
- Low response rates (often <10%)
- Younger voters underrepresented

**Nepal context**: 87% mobile penetration makes RDD viable, but rural areas with poor coverage may be undersampled.

### Face-to-Face Household Surveys

**Method**: Trained enumerators visit randomly selected households.

**Pros**:
- Higher response rates (40-60%)
- Can reach phone-less populations
- Better demographic representation

**Cons**:
- Expensive (10-20x cost of phone surveys)
- Time-consuming
- Interviewer bias possible

**Nepal context**: Gold standard for national polls. Most reliable firms use stratified cluster sampling with in-person interviews.

### Online Panels

**Method**: Recruit respondents via internet, survey via web form.

**Pros**: Fast, cheap, large samples possible

**Cons**:
- Severe selection bias (only 60% internet penetration in Nepal)
- Self-selected respondents
- Skews young, urban, educated

**Nepal context**: Unreliable for national estimates. May work for urban youth voter studies.

## Weighting: Correcting Sample Imbalances

Even random samples often over- or under-represent certain groups. **Weighting** corrects this.

**Example problem**: Your 1,000-person sample is:
- 65% male, 35% female (Nepal electorate is 51% female)
- 45% Kathmandu Valley, 55% elsewhere (Valley is only 15% of population)

**Solution**: Assign weights to make sample match population.

```
Female responses: weight = 0.51/0.35 = 1.46 (count each 1.46x)
Male responses: weight = 0.49/0.65 = 0.75 (count each 0.75x)
```

**Common weighting factors:**
- Gender
- Age group
- Province
- Urban/rural
- Education level
- Caste/ethnicity

**Risk**: Over-weighting can amplify noise. A poll with 50 young voters (5% of sample) weighted to represent 23% of electorate is unreliable.

**Red flag**: If a poll doesn't disclose weighting methodology, be skeptical.

## Question Wording: How Framing Changes Results

**Same question, different frames:**

**Version A**: "If elections were held today, which party would you vote for?"
**Version B**: "Which party do you currently support?"
**Version C**: "Which party best represents your interests?"

These produce **different results**. Version A (vote intention) is standard, but some polls use softer language that inflates support.

**Problem questions:**

❌ "Don't you think NC has done a good job?" (leading question)
❌ "Would you support UML or another party?" (false dichotomy)
❌ "Rank your top 3 parties" (not how voting works)

**Quality polls**:
✅ Use neutral language
✅ Randomize candidate order
✅ Include "undecided" option
✅ Ask voting certainty ("How certain are you? Very/somewhat/not very certain")

## Timing: When the Poll Was Conducted Matters

**Example**: Poll conducted August 15-22, published September 5.

**Published date ≠ field date**. Data may be two weeks old.

In fast-moving campaigns, polls date quickly:
- Scandals break
- Debates happen
- News cycles shift

**Rule**: Always check field dates, not publication dates.

**Nepal election timeline context:**
- 3 months before election: General sentiment, high uncertainty
- 1 month before: Campaign effects visible, still volatile
- 1 week before: Most predictive, but last-minute shifts possible
- Final 48 hours: Many voters still deciding (polling often banned)

## Response Rate: The Silent Crisis in Polling

**Response rate** = (Completed surveys / Total contacts attempted)

**Historical trend:**
- 1990s: 35-40% response rates
- 2000s: 20-25%
- 2010s: 10-15%
- 2020s: 5-10%

**Problem**: Non-response bias. People who answer surveys differ from those who don't.

**Who's underrepresented:**
- Young men (screen calls)
- Urban professionals (too busy)
- Very poor (no phone/time)
- Politically disengaged

**Who's overrepresented:**
- Elderly (answer landlines)
- Politically engaged
- People with strong opinions

**Pollster strategies:**
- Call-backs (multiple attempts)
- Evening/weekend calling
- Incentives (though this creates other biases)

**Red flag**: Polls that don't disclose response rates are hiding something.

## House Effects: Why Different Polls Show Different Results

**Phenomenon**: Poll from Firm A consistently shows NC +5%, while Firm B shows UML +3%, even when surveying simultaneously.

**Causes:**
- Different sampling frames (phone vs in-person)
- Different weighting models
- Different question wording
- Different likely voter screens

**"House effect"**: Each polling firm's systematic tendency to favor certain parties.

**Not necessarily bias**: Different methodological choices produce different but equally valid estimates.

**How to handle**:
1. Follow **trends within a pollster**, not absolute numbers
2. Look at **polling averages** across multiple firms
3. Recognize **house effects** in historical data

## Likely Voter Models: Who Will Actually Vote?

**Challenge**: Not everyone who says they'll vote actually votes. Turnout in Nepal ranges from 55-65%.

**Likely voter screen**: Questions to identify who will probably vote:

- "How certain are you to vote? Absolutely certain/probably/maybe/won't vote"
- "Did you vote in the last election?"
- "How much attention are you paying to the campaign?"
- "Do you know where your polling station is?"

**Model types:**

**Strict screen**: Only count "absolutely certain" voters who voted before
- More accurate close to election
- Smaller sample (lower precision)

**Loose screen**: Include "probably" voters
- Larger sample
- May overestimate turnout

**No screen**: Survey all eligible voters
- Often overstates parties popular with low-propensity voters

**Nepal context**: Youth voters say they'll vote at higher rates than they actually do. Polls without likely voter screens may overestimate RSP and underestimate traditional parties with older, more reliable voters.

## Aggregating Polls: The Wisdom of Crowds

**Single poll**: ±3.5% margin of error

**Average of 5 polls**: Effective margin of error shrinks to ~±1.5%

**Poll aggregation methods:**

**Simple average**: Add all polls, divide by number of polls
- Easy but treats old and new polls equally

**Weighted average**: Recent polls count more
- More responsive to trends
- Can be gamed by pollster spam (many polls from one firm)

**Regression models**: Account for house effects, poll quality, time decay
- More sophisticated
- Requires statistical expertise

**Our aggregation methodology**:
1. Include only polls with disclosed methodology
2. Weight by sample size and recency
3. Adjust for known house effects
4. Uncertainty increases for older polls

## Red Flags: How to Spot Bad Polls

**Immediate disqualifiers:**

❌ No disclosed sample size
❌ No margin of error
❌ No field dates
❌ No methodology description
❌ Sponsor with obvious partisan agenda
❌ Results published without crosstabs

**Warning signs:**

⚠️ Sample <400 (error >5%)
⚠️ Online-only methodology
⚠️ Leading questions
⚠️ No undecided option
⚠️ Round numbers (suggests fabrication)
⚠️ Results drastically different from all other polls

**Quality indicators:**

✅ Transparent methodology
✅ Appropriate sample size (1,000-3,000)
✅ Random sampling with clear frame
✅ Disclosed weighting
✅ Crosstabs available
✅ Track record of accuracy

## Nepal-Specific Polling Challenges

### Geographic Diversity

**Challenge**: Nepal's terrain makes random sampling difficult. Mountain constituencies are expensive to reach.

**Result**: Polls may undersample remote areas, which vote differently than accessible regions.

**Solution**: Stratified sampling by province/ecology with proportional allocation.

### Multiparty System

**Challenge**: With 8-10 significant parties, small sample sizes produce large errors for minor parties.

**Example**: A party at 5% support with ±3.5% MoE could actually be at 1.5% or 8.5%—a 5.7x difference!

**Result**: Seat projections become highly uncertain due to compounding errors.

### Coalition Dynamics

**Challenge**: Polls measure vote intention, but alliances change seat outcomes.

**2017 example**: UML-Maoist alliance polled similarly to 2022 as separate parties, but alliance yielded very different seat outcomes.

**Solution**: Poll both vote intention AND alliance preferences separately.

### Undecided Voters

**Nepal pattern**: 15-25% undecided until final week.

**Comparison**: UK/US typically 5-10% undecided.

**Implication**: Early polls have limited predictive value. Focus on trends, not point estimates.

## Case Study: 2022 Election Polling Accuracy

**Final pre-election poll average** (Nov 10-15, 2022):
- NC: 29%
- UML: 26%
- Maoist: 14%
- RSP: 13%
- Others: 18%

**Actual PR vote share** (Nov 20, 2022):
- NC: 26.6%
- UML: 26.6%
- Maoist: 12.8%
- RSP: 13.1%
- Others: 20.9%

**Accuracy:**
- NC: Overestimated by 2.4%
- UML: Underestimated by 0.6%
- Maoist: Overestimated by 1.2%
- RSP: Overestimated by 0.1%

**Average error: 1.1%** — well within margin of error!

**What polls got wrong**:
- Underestimated "Others" vote fragmentation
- Overestimated NC's late surge

**What polls got right**:
- RSP emergence as significant force
- Tight NC-UML race
- Maoist decline from 2017

## Best Practices for Poll Consumers

**1. Check methodology before trusting numbers**
- Sample size >1,000
- Random sampling
- Disclosed weighting
- In-person or mixed-mode

**2. Read the margin of error**
- Is the "lead" larger than MoE?
- Do confidence intervals overlap?

**3. Look at trends, not single polls**
- Is support rising/falling?
- Consistent across pollsters?

**4. Consider timing**
- How old is the data?
- What events happened since fieldwork?

**5. Aggregate multiple polls**
- Average reduces noise
- Outliers become obvious

**6. Remember uncertainty**
- Polls measure current sentiment
- Voters change minds
- Turnout is unpredictable
- Seat allocation adds complexity

## Conclusion: Polls Are Tools, Not Crystal Balls

Good polls, properly interpreted, provide valuable insights into electoral sentiment. They help campaigns allocate resources, media frame coverage, and voters make strategic decisions.

But polls are:
- **Snapshots**, not predictions
- **Estimates**, not facts
- **Probabilistic**, not certain

The next time you see a headline screaming "NC Surges to 10-Point Lead!", check the fine print. You might find the margin of error makes it a statistical tie.

Informed poll literacy makes you a more sophisticated political observer—and harder to manipulate with cherry-picked numbers.

---

*Methodology note: All examples use real polling principles but hypothetical Nepal data for illustration. Actual 2022 polling data aggregated from multiple published sources.*
