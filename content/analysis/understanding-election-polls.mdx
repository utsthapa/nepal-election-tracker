---
title: "That Poll Showing NC Ahead By 4 Points? It Might Actually Be A Tie."
excerpt: "Election polls are everywhere in Nepal's campaign season. Most voters don't know how to read them. Here's how to tell signal from noise — and why the margin of error is your best friend."
author: "Data Journalism Team"
date: "2026-02-17"
category: "Analysis"
tags: ["polling", "methodology", "statistics", "data-literacy", "elections"]
featuredImage: ""
featured: false
---

# That Poll Showing NC Ahead By 4 Points? It Might Actually Be A Tie.

Here's a scenario you've probably seen: A news outlet runs a banner headline — "NC Leads UML By 4 Points!" — and Twitter lights up. NC supporters celebrate. UML supporters cry foul. Pundits start explaining what it all means.

Then you scroll down to the fine print. Sample size: 1,200. Margin of error: ±3.5%. Confidence level: 95%.

And just like that, the 4-point "lead" could be anything from an 11-point blowout to a 3-point UML advantage. The headline told you almost nothing. The fine print told you everything.

Polls aren't broken. But the way we talk about them is. And in a country where coalition math can swing on a few thousand votes in the right constituencies, misreading a poll isn't just sloppy — it's dangerous. So let's fix that.

## What a poll actually measures (and what it doesn't)

Start with the basics. An election poll tries to estimate what 17.9 million eligible Nepali voters think by asking a tiny fraction of them — usually somewhere between 1,000 and 3,000 people.

That sounds absurd. How can 1,000 people speak for 18 million?

Math. That's how.

The margin of error in a poll depends almost entirely on sample size, not population size. Whether you're polling Nepal or India or the United States, 1,000 randomly selected respondents gets you roughly the same precision. The formula is straightforward:

**Margin of Error = 1.96 × √(p(1-p)/n)**

Where *p* is the proportion you're estimating and *n* is your sample size. The 1.96 comes from the z-score for 95% confidence. Don't worry about the math. Worry about the results:

| Sample Size | Margin of Error (95% CI) |
|-------------|--------------------------|
| 400         | ±4.9%                    |
| 600         | ±4.0%                    |
| 1,000       | ±3.1%                    |
| 1,500       | ±2.5%                    |
| 2,500       | ±2.0%                    |
| 4,000       | ±1.5%                    |

Notice something? Doubling your sample from 1,000 to 2,000 only shaves about a percentage point off the error. Quadrupling it to 4,000 gets you to 1.5%. The returns diminish fast, which is why most serious polling firms don't bother going past 2,500 or so. It's just not worth the money.

But here's the catch — and it's a big one. All of this assumes the sample is truly random and representative. That assumption is where polls live or die.

## The three numbers you should actually care about

Every reputable poll gives you three things:

**The point estimate.** NC: 32%. This is the headline number. It's also the least useful number on its own.

**The margin of error.** ±3.5%. This tells you the range of plausible values. NC's real support could be anywhere from 28.5% to 35.5%.

**The confidence level.** 95%. This means if you ran the same survey 100 times with 100 different random samples, about 95 of them would produce results within that margin of error.

Put them together and you get: "We're 95% confident NC's true support is between 28.5% and 35.5%."

Quick clarification, because this trips up even smart people: the 95% confidence level does *not* mean there's a 95% chance the true value falls in that range. The true value is fixed — we just don't know what it is. The confidence interval reflects our uncertainty about *our measurement*, not about reality.

And that other 5%? That's why outlier polls happen. One in twenty polls will produce a result outside the margin of error purely by chance. No conspiracy needed. Just statistics doing statistics things.

## When a "lead" isn't really a lead

Let's say a poll drops with these numbers:

- NC: 32% ± 3.5%
- UML: 28% ± 3.5%
- Maoist Centre: 15% ± 3.5%
- RSP: 12% ± 3.5%

The headline writes itself: NC leads UML by 4 points. But look at the confidence intervals:

- NC: 28.5% to 35.5%
- UML: 24.5% to 31.5%

Those ranges overlap. A lot. NC could actually be at 29% and UML at 31%. Or NC could be at 35% and UML at 25%. The poll can't tell you which.

When confidence intervals overlap like this, the honest thing to say is: it's a statistical tie. The race is too close to call. But "too close to call" doesn't generate clicks, so you get "NC SURGES AHEAD" instead.

Here's a rough rule of thumb: if the gap between two candidates is less than twice the margin of error, you probably can't distinguish them statistically. A 4-point lead with a ±3.5% margin? That's noise, not signal.

## The part that actually matters: how they picked the sample

Sample size gets all the attention. But sampling methodology is what separates a useful poll from expensive garbage.

Think of it this way. Sample size determines *precision* — how tight your confidence interval is. Sampling methodology determines *accuracy* — whether your estimate is centered on the right number or systematically off.

A poll of 10,000 people drawn entirely from Kathmandu Facebook users is precise and wrong. A poll of 1,000 people drawn randomly from voter rolls across all seven provinces is less precise and much more right.

### Phone surveys

Random digit dialing — where a computer generates phone numbers and calls them — is the cheapest way to reach a lot of people fast. Nepal's 87% mobile penetration makes it viable. But there are problems.

Response rates are abysmal. We're talking single digits in many cases. And the people who pick up the phone and agree to answer questions for 15 minutes are not a random cross-section of Nepal. They skew older, more politically engaged, and more opinionated. Young men screen their calls. Urban professionals are too busy. The very poor may not have reliable phone access.

### Face-to-face surveys

This is the gold standard for Nepal. Trained enumerators visit randomly selected households, sit down, and conduct interviews. Response rates run 40-60%, which is worlds better than phone surveys. You can reach people without phones. You can verify demographics in real time.

The downside? Cost. In-person surveys run 10 to 20 times more expensive than phone polls. And in a country where getting to some constituencies requires days of walking, the logistics are brutal. Mountain districts are chronically undersampled for exactly this reason.

### Online panels

Fast. Cheap. And in Nepal, basically useless for national estimates. With internet penetration around 60% and heavily skewed toward young, urban, educated populations, an online poll tells you what Kathmandu millennials think. Which is fine if that's your question. It's not fine if you're trying to estimate national vote share.

## Weighting: the fudge factor that makes polls work (or doesn't)

Even good random samples end up lopsided. Maybe your phone survey reached 65% men and 35% women, but Nepal's electorate is 51% female. Maybe 45% of your respondents live in the Kathmandu Valley, which is only about 15% of the population.

Weighting fixes this. You assign a multiplier to underrepresented groups so the sample matches known population parameters.

The math is simple. If women are 51% of the electorate but 35% of your sample, each female respondent gets a weight of 1.46 (0.51 divided by 0.35). Each male respondent gets downweighted to 0.75.

Pollsters typically weight on gender, age, province, urban/rural split, education, and sometimes caste/ethnicity. Done well, weighting corrects real imbalances. Done poorly, it amplifies noise.

Here's the danger zone: if your sample only has 50 respondents aged 18-24 (5% of the sample) but you're weighting them up to represent 23% of the electorate, you're building a skyscraper on a foundation of sand. Those 50 people are carrying an enormous amount of weight, and any quirks in that tiny group get magnified across the whole estimate.

**Red flag**: If a poll doesn't tell you how it weighted the data, be skeptical. If it doesn't tell you *whether* it weighted the data, be very skeptical.

## Question wording changes everything

This is one of those things that seems obvious once you hear it but gets ignored constantly.

Ask "If elections were held today, which party would you vote for?" and you get one answer. Ask "Which party do you currently support?" and you get a different answer. Ask "Which party best represents your interests?" and you get yet another answer.

The first version — vote intention — is the standard. But some polls use softer language that inflates support numbers, and they don't always flag the difference.

Worse are the leading questions. "Don't you think NC has done a good job?" isn't a poll question. It's a push poll. "Would you support UML or another party?" presents a false choice. These aren't measuring opinion. They're manufacturing it.

Quality polls use neutral language, randomize the order candidates appear (because people tend to pick the first option they see), include an "undecided" option, and ask follow-up questions about how certain respondents are about their choice. That last part matters a lot, because someone who says "I guess NC?" is very different from someone who says "Absolutely NC, no question."

## The timing trap

Here's something that catches people constantly: the date a poll is published is not the date the poll was conducted.

A poll might hit the news on September 5, but the actual interviews happened August 15-22. That's two weeks of political events — scandals, debates, endorsements, gaffes — that the poll doesn't capture. In a fast-moving campaign, two weeks is an eternity.

Always check the field dates. Always.

And timing matters in a bigger sense too. The predictive value of polls changes dramatically depending on when they're conducted:

**Three months out**: You're measuring general vibes. Uncertainty is enormous. Treat these as background noise.

**One month out**: Campaign effects are starting to show. Trends are more meaningful than point estimates.

**One week out**: This is where polls are most useful. But last-minute shifts can still happen.

**Final 48 hours**: Many voters are still making up their minds. And in Nepal, polling is often banned in this window anyway.

## The response rate crisis nobody talks about

Response rates have been collapsing for decades. In the 1990s, about 35-40% of people contacted for a survey would actually complete it. By the 2020s, we're down to 5-10% in many cases.

Think about what that means. For every 100 people a pollster tries to reach, 90 to 95 of them hang up, don't answer, or refuse to participate. The poll's results are based entirely on the 5-10 who said yes.

Are those 5-10 people representative of the other 90-95? Almost certainly not. People who agree to take surveys tend to be older, more politically engaged, and more opinionated. Young men who screen every unknown call? Not in the sample. Busy professionals? Not in the sample. People who distrust institutions and don't want to share their political views? Definitely not in the sample.

This is called non-response bias, and it's the single biggest threat to polling accuracy today. Pollsters try to compensate with callbacks, evening and weekend calling, and weighting adjustments. But you can only correct for biases you know about. The unknown unknowns are what keep good pollsters up at night.

If a poll doesn't disclose its response rate, that's a red flag. They might be hiding a number that would make you question the whole exercise.

## House effects: why Firm A and Firm B never agree

You've probably noticed this: one polling firm consistently shows NC ahead, while another consistently shows UML with an edge. They're polling at the same time, in the same country, about the same election. What gives?

Different methodological choices produce different results. Firm A uses phone surveys; Firm B goes door-to-door. Firm A weights heavily on education; Firm B prioritizes geographic balance. Firm A uses a strict likely voter screen; Firm B counts anyone who says they'll probably vote.

None of these choices are necessarily wrong. They're just different. And they produce systematic differences — what pollsters call "house effects" — that show up election after election.

So how do you deal with this? Three ways.

First, track trends within a single pollster rather than comparing absolute numbers across firms. If Firm A showed NC at 34% last month and 30% this month, that 4-point drop means something regardless of whether Firm A runs high on NC.

Second, look at polling averages. When you combine five or six polls from different firms, the house effects tend to cancel out. A single poll has a margin of error around ±3.5%. Average five polls together and the effective error drops to roughly ±1.5%.

Third, if you're really serious about this, look at each firm's historical accuracy. Which ones got closest to the actual result in 2017? In 2022? Past performance doesn't guarantee future accuracy, but it's the best predictor we have.

## Likely voter models: the hidden assumption that changes everything

Here's a problem. Turnout in Nepal's elections ranges from about 55% to 65%. That means somewhere between 35% and 45% of eligible voters stay home. But when a pollster calls someone and asks "Who would you vote for?", almost everyone gives an answer. People who won't actually show up on election day still have opinions.

If you count all of them equally, you're going to get a skewed picture. Parties that are popular with unreliable voters — younger demographics, first-time voters, people who are vaguely sympathetic but not motivated enough to stand in line — will look stronger than they actually are.

This is where likely voter screens come in. Pollsters ask a series of questions designed to figure out who's actually going to vote:

- How certain are you to vote? (Absolutely certain / Probably / Maybe / Won't vote)
- Did you vote in the last election?
- How closely are you following the campaign?
- Do you know where your polling station is?

A strict screen only counts people who say "absolutely certain" and voted last time. This gives you a smaller but more realistic sample. A loose screen includes the "probably" crowd, which gives you more data but might overestimate turnout.

And then there's no screen at all, which just counts everyone. This tends to overstate support for newer parties that excite people who don't reliably vote.

**The Nepal-specific wrinkle**: Young voters consistently tell pollsters they plan to vote at higher rates than they actually do. This means polls without a likely voter screen may overestimate parties like RSP, which draws heavily from younger demographics, and underestimate traditional parties like NC and UML, whose supporters are older and more habitual about showing up.

We don't know exactly how big this effect is. But it's real, and it's one reason why RSP's actual performance sometimes comes in below what pre-election polls suggest.

## How the 2022 polls actually did

Let's ground all of this in a real example. Here's what the final pre-election polling averages looked like in mid-November 2022, compared to what actually happened on election day:

| Party | Final Poll Average (Nov 10-15) | Actual PR Vote Share (Nov 20) | Error |
|-------|-------------------------------|-------------------------------|-------|
| NC | 29% | 26.6% | +2.4% |
| UML | 26% | 26.6% | -0.6% |
| Maoist Centre | 14% | 12.8% | +1.2% |
| RSP | 13% | 13.1% | -0.1% |
| Others | 18% | 20.9% | -2.9% |

Average error across parties: about 1.4 percentage points. That's good! Well within the margin of error for individual polls, and better than a lot of polling in established democracies.

The polls got the big picture right. They correctly identified the tight NC-UML race. They nailed RSP's arrival as a serious force — a genuinely hard thing to poll, since RSP was a brand-new party with no historical baseline. They caught the Maoist Centre's continued decline from 2017.

Where they stumbled was on the "Others" category — the fragmented collection of smaller parties that collectively grabbed nearly 21% of the PR vote. Polls underestimated this by about 3 points, which makes sense: it's hard to poll for a dozen small parties individually, and respondents may feel social pressure to name a major party rather than admit they're voting for a local or regional outfit.

The NC overestimate is interesting too. There was a lot of late campaign coverage suggesting NC was pulling ahead, and it's possible polls picked up a "bandwagon effect" that didn't fully materialize at the ballot box. Or maybe a chunk of soft NC supporters broke toward smaller parties in the final days. We can't say for sure.

## Nepal's specific polling headaches

Polling is hard everywhere. Polling Nepal is harder than most places, for reasons that are worth spelling out.

### The geography problem

Nepal's terrain is, to put it mildly, not pollster-friendly. Getting a representative sample means reaching mountain constituencies where the nearest road might be a two-day walk away. Most polling firms handle this through stratified cluster sampling — dividing the country into geographic strata (province, ecological zone, urban/rural) and then randomly selecting clusters within each stratum.

But even with good stratification, remote areas tend to be undersampled because they're expensive and time-consuming to reach. And these areas vote differently. Mountain constituencies have different party loyalties, different turnout patterns, different demographic profiles than the Kathmandu Valley or the Madhesh plains. Undersampling them introduces a systematic bias that weighting can only partially fix.

### The multiparty problem

Nepal doesn't have a two-party system. It has eight to ten parties that win seats in any given election. And this creates a statistical headache.

When a party polls at 5% with a ±3.5% margin of error, its true support could be anywhere from 1.5% to 8.5%. That's a nearly sixfold range. For seat projections, this is devastating. The difference between 2% and 8% of the vote could be the difference between zero seats and fifteen seats, depending on the district.

This means that while polls can give you a reasonably clear picture of the top two or three parties, they're essentially guessing when it comes to smaller parties. And in a system where coalition math depends on exactly how many seats the fourth- and fifth-place parties win, that uncertainty cascades through every prediction.

### The coalition problem

Polls measure vote intention. But in Nepal, what matters for government formation is seat outcomes, and seat outcomes depend heavily on alliance structures.

The 2017 election is the perfect example. UML and the Maoists ran together as an alliance, and the combined ticket dominated FPTP constituencies. In 2022, they ran separately, and the seat distribution looked completely different even though the underlying vote shares weren't radically changed.

A good poll needs to capture not just "who would you vote for?" but "how would alliance configurations change your vote?" Those are different questions with different answers, and most polls only ask the first one.

### The undecided voter problem

In Nepal, 15-25% of voters remain undecided until the final week of the campaign. Compare that to the UK or US, where the undecided share is typically 5-10% at the same stage.

This means early polls in Nepal have limited predictive value. A poll three months before the election is measuring vibes, not outcomes. Even a poll one month out is capturing a snapshot that could shift dramatically as a quarter of the electorate makes up its mind.

The honest takeaway: focus on trends over time, not any single poll's point estimates. Is NC's support rising or falling across multiple polls from multiple firms? That tells you something. A single poll showing NC at 32% tells you almost nothing.

## How to spot a bad poll in 30 seconds

You don't need a statistics degree. You need a checklist.

**Instant disqualifiers** — if any of these are missing, ignore the poll entirely:

- No disclosed sample size
- No margin of error reported
- No field dates
- No description of methodology
- Commissioned by a party or partisan organization with no independent oversight
- Results released without crosstabs (the demographic breakdowns)

**Warning signs** — these don't automatically disqualify a poll, but they should make you cautious:

- Sample under 400 (margin of error above 5%)
- Online-only methodology in a country with 60% internet penetration
- Leading or loaded questions
- No "undecided" option offered
- Suspiciously round numbers (30.0%, 25.0%, 15.0% — real data is messier than that)
- Results that are wildly different from every other poll conducted at the same time

**Good signs** — these suggest the pollsters know what they're doing:

- Transparent methodology section
- Sample of 1,000-3,000 with clear sampling frame
- Random or stratified random sampling
- Disclosed weighting procedures
- Crosstabs available for download
- Track record of reasonable accuracy in past elections

## Putting it all together

So what should you actually do the next time a poll drops?

**Step one**: Check the methodology. Sample size, sampling method, weighting, field dates. If any of that is missing, move on.

**Step two**: Read the margin of error. Is the reported "lead" bigger than the margin of error? If not, it's a statistical tie, no matter what the headline says.

**Step three**: Look at trends. One poll is an anecdote. Five polls over two months showing the same trajectory is data.

**Step four**: Consider timing. A poll from three months ago is ancient history in campaign terms. A poll from last week is useful. A poll from yesterday is gold.

**Step five**: Aggregate. Average multiple polls from different firms. The house effects and random noise tend to wash out, and you're left with something closer to the truth.

**Step six** — and this is the hardest one — **sit with the uncertainty**. A poll showing NC at 32% doesn't mean NC is at 32%. It means NC is probably somewhere between 28% and 36%, and even that "probably" only covers 95 out of 100 scenarios.

## What we know, and what we don't

Here's what we can say with confidence: polling in Nepal has gotten better. The 2022 results showed average errors around 1.4 percentage points — genuinely solid performance. The big stories (RSP's rise, the NC-UML dead heat, the Maoist decline) were all captured in pre-election surveys.

Here's what we can't say: whether that accuracy will hold in 2027. Every election is different. New parties create new challenges. Alliance structures shift. Voter behavior evolves. The 2022 polls benefited from a relatively stable campaign environment. A more volatile race could produce bigger misses.

And here's the thing that should keep all of us honest: polls are snapshots, not prophecies. They tell you where things stand right now, within a range of uncertainty, based on a set of assumptions that may or may not hold. They're the best tool we have for reading public opinion between elections. They're also imperfect, sometimes badly so.

The next time you see "NC Surges to 10-Point Lead!" in a headline, don't celebrate or despair. Check the margin of error. Check the sample. Check the methodology.

You might find the surge is just noise.

---

*Methodology note: All polling principles discussed use standard statistical frameworks. The 2022 polling accuracy data is aggregated from multiple published pre-election surveys. Hypothetical examples use realistic but illustrative numbers for Nepal's electoral context.*