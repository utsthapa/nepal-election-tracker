---
title: 'That Poll Showing NC Ahead By 4 Points? It Might Actually Be A Tie.'
excerpt: "Election polls are everywhere in Nepal's campaign season. Most voters don't know how to read them. Here's how to tell signal from noise — and why the margin of error is your best friend."
author: 'Data Journalism Team'
date: '2026-02-17'
category: 'Analysis'
tags: ['polling', 'methodology', 'statistics', 'data-literacy', 'elections']
featuredImage: ''
featured: false
---

# That Poll Showing NC Ahead By 4 Points? It Might Actually Be A Tie.

<KeyFindings>
**KEY FINDINGS**

- A 4-point lead with a ±3.5% margin of error is statistically indistinguishable from a tie — confidence intervals overlap
- Sample size matters less than sampling methodology — 1,000 well-chosen respondents beats 10,000 poorly-chosen ones
- Response rates have collapsed to 5-10%, introducing significant non-response bias that weighting can't fully fix
- Nepal's 2022 polls performed remarkably well (1.4% average error) but face unique geographical and multiparty challenges
- The most reliable insight comes from polling averages, not single polls — house effects cancel out across multiple firms
  </KeyFindings>

<PullQuote>
  Polls aren't broken. But the way we talk about them is. And in a country where coalition math can
  swing on a few thousand votes, misreading a poll isn't just sloppy — it's dangerous.
</PullQuote>

Here's a scenario you've probably seen: A news outlet runs a banner headline — "NC Leads UML By 4 Points!" — and Twitter lights up. NC supporters celebrate. UML supporters cry foul. Pundits start explaining what it all means.

Then you scroll down to the fine print. Sample size: 1,200. Margin of error: ±3.5%. Confidence level: 95%.

And just like that, the 4-point "lead" could be anything from an 11-point blowout to a 3-point UML advantage. **The headline told you almost nothing. The fine print told you everything.**

---

## What a Poll Actually Measures

An election poll tries to estimate what 17.9 million eligible Nepali voters think by asking a tiny fraction of them — usually somewhere between 1,000 and 3,000 people.

That sounds absurd. How can 1,000 people speak for 18 million? **Math. That's how.**

The margin of error in a poll depends almost entirely on sample size, not population size. Whether you're polling Nepal or India or the United States, 1,000 randomly selected respondents gets you roughly the same precision.

**Margin of Error = 1.96 × √(p(1-p)/n)**

Where _p_ is the proportion you're estimating and _n_ is your sample size. The 1.96 comes from the z-score for 95% confidence.

<Callout type="info" title="SAMPLE SIZE VS. PRECISION">
  | Sample Size | Margin of Error (95% CI) | |-------------|--------------------------| | 400 |
  ±4.9% | | 600 | ±4.0% | | 1,000 | ±3.1% | | 1,500 | ±2.5% | | 2,500 | ±2.0% | | 4,000 | ±1.5% |
</Callout>

Doubling your sample from 1,000 to 2,000 only shaves about a percentage point off the error. **The returns diminish fast**, which is why most serious polling firms don't bother going past 2,500 or so.

But here's the catch — and it's a big one. All of this assumes the sample is truly random and representative. That assumption is where polls live or die.

---

## The Three Numbers That Matter

Every reputable poll gives you three things:

**The point estimate.** NC: 32%. This is the headline number. It's also the least useful number on its own.

**The margin of error.** ±3.5%. This tells you the range of plausible values. NC's real support could be anywhere from 28.5% to 35.5%.

**The confidence level.** 95%. This means if you ran the same survey 100 times with 100 different random samples, about 95 of them would produce results within that margin of error.

Put them together and you get: **"We're 95% confident NC's true support is between 28.5% and 35.5%."**

<Sidebar title="Understanding Confidence Intervals">
The 95% confidence level does **not** mean there's a 95% chance the true value falls in that range. The true value is fixed — we just don't know what it is. The confidence interval reflects our uncertainty about *our measurement*, not about reality.

Think of it this way: If you're measuring a table with a tape measure, the table's length doesn't change. Your confidence in your measurement does.

</Sidebar>

---

## When a "Lead" Isn't Really a Lead

<PullQuote>
  When confidence intervals overlap like this, the honest thing to say is: it's a statistical tie.
  The race is too close to call.
</PullQuote>

Let's say a poll drops with these numbers:

- NC: 32% ± 3.5%
- UML: 28% ± 3.5%

The headline writes itself: NC leads UML by 4 points. But look at the confidence intervals:

- NC: 28.5% to 35.5%
- UML: 24.5% to 31.5%

Those ranges overlap. **A lot.** NC could actually be at 29% and UML at 31%. Or NC could be at 35% and UML at 25%. The poll can't tell you which.

Here's a rough rule of thumb: **if the gap between two candidates is less than twice the margin of error, you probably can't distinguish them statistically.** A 4-point lead with a ±3.5% margin? That's noise, not signal.

---

## Sampling: What Actually Separates Good Polls From Bad

<PullQuote>Sample size determines precision. Sampling methodology determines accuracy.</PullQuote>

Sample size gets all the attention. But sampling methodology is what separates a useful poll from expensive garbage.

A poll of 10,000 people drawn entirely from Kathmandu Facebook users is precise and wrong. A poll of 1,000 people drawn randomly from voter rolls across all seven provinces is less precise and much more right.

**Three main approaches:**

1. **Phone surveys** — Random digit dialing is the cheapest way to reach people fast. Nepal's 87% mobile penetration makes it viable. But response rates are abysmal — single digits in many cases. The people who pick up skew older, more politically engaged, and more opinionated.

2. **Face-to-face surveys** — This is the gold standard for Nepal. Trained enumerators visit randomly selected households. Response rates run 40-60%. You can reach people without phones and verify demographics in real time. The downside? Cost. In-person surveys run 10 to 20 times more expensive than phone polls.

3. **Online panels** — Fast. Cheap. And in Nepal, basically useless for national estimates. With internet penetration around 60% and heavily skewed toward young, urban, educated populations, an online poll tells you what Kathmandu millennials think. Which is fine if that's your question. It's not fine if you're trying to estimate national vote share.

---

## Weighting: The Fudge Factor

Even good random samples end up lopsided. Maybe your phone survey reached 65% men and 35% women, but Nepal's electorate is 51% female.

Weighting fixes this. You assign a multiplier to underrepresented groups so the sample matches known population parameters.

If women are 51% of the electorate but 35% of your sample, each female respondent gets a weight of 1.46. Each male respondent gets downweighted to 0.75.

**The danger zone**: If your sample only has 50 respondents aged 18-24 (5% of the sample) but you're weighting them up to represent 23% of the electorate, you're building a skyscraper on a foundation of sand.

<PullQuote>
  Red flag: If a poll doesn't tell you how it weighted the data, be skeptical. If it doesn't tell
  you whether it weighted the data, be very skeptical.
</PullQuote>

---

## Question Wording Changes Everything

Ask "If elections were held today, which party would you vote for?" and you get one answer. Ask "Which party do you currently support?" and you get a different answer. Ask "Which party best represents your interests?" and you get yet another answer.

The first version — vote intention — is the standard. But some polls use softer language that inflates support numbers.

Worse are the leading questions. "Don't you think NC has done a good job?" isn't a poll question. It's a push poll. "Would you support UML or another party?" presents a false choice. These aren't measuring opinion. They're manufacturing it.

Quality polls use neutral language, randomize the order candidates appear, include an "undecided" option, and ask follow-up questions about how certain respondents are about their choice.

---

## The Timing Trap

<PullQuote>
  A poll from three months ago is ancient history in campaign terms. A poll from last week is
  useful. A poll from yesterday is gold.
</PullQuote>

Here's something that catches people constantly: the date a poll is published is not the date the poll was conducted.

A poll might hit the news on September 5, but the actual interviews happened August 15-22. That's two weeks of political events — scandals, debates, endorsements, gaffes — that the poll doesn't capture. In a fast-moving campaign, two weeks is an eternity.

The predictive value of polls changes dramatically depending on when they're conducted:

**Three months out**: You're measuring general vibes. Uncertainty is enormous. Treat these as background noise.

**One month out**: Campaign effects are starting to show. Trends are more meaningful than point estimates.

**One week out**: This is where polls are most useful. But last-minute shifts can still happen.

**Final 48 hours**: Many voters are still making up their minds. And in Nepal, polling is often banned in this window anyway.

---

## The Response Rate Crisis

Response rates have been collapsing for decades. In the 1990s, about 35-40% of people contacted for a survey would actually complete it. By the 2020s, we're down to 5-10% in many cases.

Think about what that means. **For every 100 people a pollster tries to reach, 90 to 95 of them hang up, don't answer, or refuse to participate.** The poll's results are based entirely on the 5-10 who said yes.

This is called non-response bias, and it's the single biggest threat to polling accuracy today. Pollsters try to compensate with callbacks, evening and weekend calling, and weighting adjustments. But you can only correct for biases you know about. The unknown unknowns are what keep good pollsters up at night.

---

## House Effects: Why Firm A and Firm B Never Agree

You've probably noticed this: one polling firm consistently shows NC ahead, while another consistently shows UML with an edge. They're polling at the same time, in the same country, about the same election.

Different methodological choices produce different results. Firm A uses phone surveys; Firm B goes door-to-door. Firm A weights heavily on education; Firm B prioritizes geographic balance.

**How to deal with house effects:**

1. Track trends within a single pollster rather than comparing absolute numbers across firms. If Firm A showed NC at 34% last month and 30% this month, that 4-point drop means something.

2. Look at polling averages. When you combine five or six polls from different firms, the house effects tend to cancel out. A single poll has a margin of error around ±3.5%. Average five polls together and the effective error drops to roughly ±1.5%.

3. Look at each firm's historical accuracy. Which ones got closest to the actual result in 2017? In 2022? Past performance doesn't guarantee future accuracy, but it's the best predictor we have.

---

## Likely Voter Models

Turnout in Nepal's elections ranges from about 55% to 65%. That means somewhere between 35% and 45% of eligible voters stay home. But when a pollster calls someone and asks "Who would you vote for?", almost everyone gives an answer.

If you count all of them equally, you're going to get a skewed picture. Parties that are popular with unreliable voters — younger demographics, first-time voters, people who are vaguely sympathetic but not motivated enough to stand in line — will look stronger than they actually are.

<Sidebar title="How Likely Voter Models Work">
Pollsters ask a series of questions designed to figure out who's actually going to vote:

- How certain are you to vote? (Absolutely certain / Probably / Maybe / Won't vote)
- Did you vote in the last election?
- How closely are you following the campaign?
- Do you know where your polling station is?

A strict screen only counts people who say "absolutely certain" and voted last time. This gives you a smaller but more realistic sample. A loose screen includes the "probably" crowd, which gives you more data but might overestimate turnout.

**Nepal-specific wrinkle**: Young voters consistently tell pollsters they plan to vote at higher rates than they actually do. This means polls without a likely voter screen may overestimate parties like RSP, which draws heavily from younger demographics.

</Sidebar>

---

## How the 2022 Polls Actually Did

Let's ground all of this in a real example. Here's what the final pre-election polling averages looked like in mid-November 2022, compared to what actually happened on election day:

<Callout type="info" title="2022 POLLING ACCURACY">
| Party | Final Poll Average (Nov 10-15) | Actual PR Vote Share (Nov 20) | Error |
|-------|-------------------------------|-------------------------------|-------|
| NC | 29% | 26.6% | +2.4% |
| UML | 26% | 26.6% | -0.6% |
| Maoist Centre | 14% | 12.8% | +1.2% |
| RSP | 13% | 13.1% | -0.1% |
| Others | 18% | 20.9% | -2.9% |

**Average error**: 1.4 percentage points

</Callout>

The polls got the big picture right. They correctly identified the tight NC-UML race. They nailed RSP's arrival as a serious force — a genuinely hard thing to poll, since RSP was a brand-new party with no historical baseline. They caught the Maoist Centre's continued decline from 2017.

Where they stumbled was on the "Others" category — the fragmented collection of smaller parties that collectively grabbed nearly 21% of the PR vote. Polls underestimated this by about 3 points, which makes sense: it's hard to poll for a dozen small parties individually.

---

## Nepal's Specific Polling Headaches

Polling is hard everywhere. Polling Nepal is harder than most places.

**The geography problem**: Nepal's terrain is, to put it mildly, not pollster-friendly. Getting a representative sample means reaching mountain constituencies where the nearest road might be a two-day walk away. Remote areas tend to be undersampled because they're expensive and time-consuming to reach. And these areas vote differently.

**The multiparty problem**: Nepal doesn't have a two-party system. It has eight to ten parties that win seats in any given election. When a party polls at 5% with a ±3.5% margin of error, its true support could be anywhere from 1.5% to 8.5%. That's a nearly sixfold range. For seat projections, this is devastating.

**The coalition problem**: Polls measure vote intention. But in Nepal, what matters for government formation is seat outcomes, and seat outcomes depend heavily on alliance structures. A good poll needs to capture not just "who would you vote for?" but "how would alliance configurations change your vote?"

**The undecided voter problem**: In Nepal, 15-25% of voters remain undecided until the final week of the campaign. Compare that to the UK or US, where the undecided share is typically 5-10% at the same stage. The honest takeaway: **focus on trends over time, not any single poll's point estimates.**

---

## How to Spot a Bad Poll in 30 Seconds

You don't need a statistics degree. You need a checklist.

**Instant disqualifiers** — if any of these are missing, ignore the poll entirely:

1. No disclosed sample size
2. No margin of error reported
3. No field dates
4. No description of methodology
5. Commissioned by a party or partisan organization with no independent oversight
6. Results released without crosstabs (the demographic breakdowns)

**Warning signs** — these don't automatically disqualify a poll, but they should make you cautious:

- Sample under 400 (margin of error above 5%)
- Online-only methodology in a country with 60% internet penetration
- Leading or loaded questions
- No "undecided" option offered
- Suspiciously round numbers (30.0%, 25.0%, 15.0% — real data is messier than that)
- Results that are wildly different from every other poll conducted at the same time

**Good signs** — these suggest the pollsters know what they're doing:

- Transparent methodology section
- Sample of 1,000-3,000 with clear sampling frame
- Random or stratified random sampling
- Disclosed weighting procedures
- Crosstabs available for download
- Track record of reasonable accuracy in past elections

---

## Putting It All Together

<PullQuote>
  Polls are snapshots, not prophecies. They tell you where things stand right now, within a range of
  uncertainty, based on a set of assumptions that may or may not hold.
</PullQuote>

So what should you actually do the next time a poll drops?

1. **Check the methodology.** Sample size, sampling method, weighting, field dates. If any of that is missing, move on.

2. **Read the margin of error.** Is the reported "lead" bigger than the margin of error? If not, it's a statistical tie, no matter what the headline says.

3. **Look at trends.** One poll is an anecdote. Five polls over two months showing the same trajectory is data.

4. **Consider timing.** A poll from three months ago is ancient history in campaign terms. A poll from last week is useful. A poll from yesterday is gold.

5. **Aggregate.** Average multiple polls from different firms. The house effects and random noise tend to wash out, and you're left with something closer to the truth.

6. **Sit with the uncertainty.** A poll showing NC at 32% doesn't mean NC is at 32%. It means NC is probably somewhere between 28% and 36%, and even that "probably" only covers 95 out of 100 scenarios.

---

## What We Know, and What We Don't

Here's what we can say with confidence: polling in Nepal has gotten better. The 2022 results showed average errors around 1.4 percentage points — genuinely solid performance. The big stories (RSP's rise, the NC-UML dead heat, the Maoist decline) were all captured in pre-election surveys.

Here's what we can't say: whether that accuracy will hold in 2027. Every election is different. New parties create new challenges. Alliance structures shift. Voter behavior evolves.

The next time you see "NC Surges to 10-Point Lead!" in a headline, don't celebrate or despair. Check the margin of error. Check the sample. Check the methodology.

You might find the surge is just noise.

---

_Methodology note: All polling principles discussed use standard statistical frameworks. The 2022 polling accuracy data is aggregated from multiple published pre-election surveys. Hypothetical examples use realistic but illustrative numbers for Nepal's electoral context._
